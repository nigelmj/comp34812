{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference\n",
    "### Using an ensemble Architecture with Attention and Local Inference Modelling\n",
    "\n",
    "This notebook runs the model on the test set.\n",
    "Hyperparameters are set to the same values as they are in the training pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:40:56.461107Z",
     "iopub.status.busy": "2025-04-04T00:40:56.460645Z",
     "iopub.status.idle": "2025-04-04T00:40:56.465288Z",
     "shell.execute_reply": "2025-04-04T00:40:56.464339Z",
     "shell.execute_reply.started": "2025-04-04T00:40:56.461071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:40:56.466630Z",
     "iopub.status.busy": "2025-04-04T00:40:56.466371Z",
     "iopub.status.idle": "2025-04-04T00:40:56.480867Z",
     "shell.execute_reply": "2025-04-04T00:40:56.480012Z",
     "shell.execute_reply.started": "2025-04-04T00:40:56.466599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyperparameters that were selected for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:40:56.482843Z",
     "iopub.status.busy": "2025-04-04T00:40:56.482652Z",
     "iopub.status.idle": "2025-04-04T00:40:56.501057Z",
     "shell.execute_reply": "2025-04-04T00:40:56.500338Z",
     "shell.execute_reply.started": "2025-04-04T00:40:56.482827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "MAX_LENGTH = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads the specificied csv file and splits into required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:40:56.502132Z",
     "iopub.status.busy": "2025-04-04T00:40:56.501946Z",
     "iopub.status.idle": "2025-04-04T00:40:56.521445Z",
     "shell.execute_reply": "2025-04-04T00:40:56.520584Z",
     "shell.execute_reply.started": "2025-04-04T00:40:56.502116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    \"\"\"Load dataset from CSV file\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[\"premise\"].astype(str).tolist(), df[\"hypothesis\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:40:56.522548Z",
     "iopub.status.busy": "2025-04-04T00:40:56.522295Z",
     "iopub.status.idle": "2025-04-04T00:41:06.697229Z",
     "shell.execute_reply": "2025-04-04T00:41:06.696276Z",
     "shell.execute_reply.started": "2025-04-04T00:40:56.522517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the path to test data:  /kaggle/input/nlu-test-dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "test_data_path = input(\"Input the path to test data: \")\n",
    "test_premises, test_hypotheses = load_data(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokeniser that was created during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:13.918341Z",
     "iopub.status.busy": "2025-04-04T00:41:13.918109Z",
     "iopub.status.idle": "2025-04-04T00:41:13.945785Z",
     "shell.execute_reply": "2025-04-04T00:41:13.944968Z",
     "shell.execute_reply.started": "2025-04-04T00:41:13.918322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tokenizer = input(\"Input the path to tokenizer: \")\n",
    "with open(tokenizer, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise the input sequences and pad them to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:13.946925Z",
     "iopub.status.busy": "2025-04-04T00:41:13.946606Z",
     "iopub.status.idle": "2025-04-04T00:41:13.950797Z",
     "shell.execute_reply": "2025-04-04T00:41:13.949998Z",
     "shell.execute_reply.started": "2025-04-04T00:41:13.946877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(texts, tokenizer, max_length):\n",
    "    \"\"\"Convert text to sequences and pad\"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:13.953492Z",
     "iopub.status.busy": "2025-04-04T00:41:13.953275Z",
     "iopub.status.idle": "2025-04-04T00:41:14.050274Z",
     "shell.execute_reply": "2025-04-04T00:41:14.049408Z",
     "shell.execute_reply.started": "2025-04-04T00:41:13.953473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test_p = tokenize_and_pad(test_premises, tokenizer, MAX_LENGTH)\n",
    "X_test_h = tokenize_and_pad(test_hypotheses, tokenizer, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use the GloVe 6B, 300 dimensional embeddings file for the pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:14.051797Z",
     "iopub.status.busy": "2025-04-04T00:41:14.051525Z",
     "iopub.status.idle": "2025-04-04T00:41:14.055479Z",
     "shell.execute_reply": "2025-04-04T00:41:14.054613Z",
     "shell.execute_reply.started": "2025-04-04T00:41:14.051765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:14.056622Z",
     "iopub.status.busy": "2025-04-04T00:41:14.056319Z",
     "iopub.status.idle": "2025-04-04T00:41:14.070380Z",
     "shell.execute_reply": "2025-04-04T00:41:14.069601Z",
     "shell.execute_reply.started": "2025-04-04T00:41:14.056593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    \"\"\"\n",
    "    Load the embeddings for every word in the file\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:14.071335Z",
     "iopub.status.busy": "2025-04-04T00:41:14.071093Z",
     "iopub.status.idle": "2025-04-04T00:41:44.624794Z",
     "shell.execute_reply": "2025-04-04T00:41:44.623845Z",
     "shell.execute_reply.started": "2025-04-04T00:41:14.071315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the path to glove embeddings file:  /kaggle/input/glove300/glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "glove_path = input(\"Input the path to glove embeddings file: \")\n",
    "embeddings_index = load_glove_embeddings(glove_path, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training, create the embedding matrix for every word in the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:44.649518Z",
     "iopub.status.busy": "2025-04-04T00:41:44.649238Z",
     "iopub.status.idle": "2025-04-04T00:41:44.653706Z",
     "shell.execute_reply": "2025-04-04T00:41:44.652889Z",
     "shell.execute_reply.started": "2025-04-04T00:41:44.649494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding_index, word_index, embedding_dim, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix from the GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        if index >= vocab_size:\n",
    "            break\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:44.654831Z",
     "iopub.status.busy": "2025-04-04T00:41:44.654625Z",
     "iopub.status.idle": "2025-04-04T00:41:44.748975Z",
     "shell.execute_reply": "2025-04-04T00:41:44.748280Z",
     "shell.execute_reply.started": "2025-04-04T00:41:44.654813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = create_embedding_matrix(embeddings_index, word_index, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify custom functions used in the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:44.755004Z",
     "iopub.status.busy": "2025-04-04T00:41:44.754701Z",
     "iopub.status.idle": "2025-04-04T00:41:44.768735Z",
     "shell.execute_reply": "2025-04-04T00:41:44.767975Z",
     "shell.execute_reply.started": "2025-04-04T00:41:44.754974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def soft_attention(premise, hypothesis):\n",
    "    attention = Dot(axes=-1)([premise, hypothesis])\n",
    "    premise_attn = Softmax(axis=-1)(attention)\n",
    "    hypothesis_attn = Softmax(axis=-2)(attention)\n",
    "\n",
    "    premise_aligned = Dot(axes=1)([premise_attn, hypothesis])\n",
    "    hypothesis_aligned = Dot(axes=1)([hypothesis_attn, premise])\n",
    "    return premise_aligned, hypothesis_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify paths to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:41:44.770000Z",
     "iopub.status.busy": "2025-04-04T00:41:44.769698Z",
     "iopub.status.idle": "2025-04-04T00:42:08.943623Z",
     "shell.execute_reply": "2025-04-04T00:42:08.942973Z",
     "shell.execute_reply.started": "2025-04-04T00:41:44.769970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the path to LSTM model:  /kaggle/input/ensemble/tensorflow2/default/1/nli_LSTM_model.keras\n",
      "Input the path to BiLSTM model:  /kaggle/input/ensemble/tensorflow2/default/1/nli_BiLSTM_model.keras\n",
      "Input the path to GRU model:  /kaggle/input/ensemble/tensorflow2/default/1/nli_GRU_model.keras\n",
      "Input the path to BiGRU model:  /kaggle/input/ensemble/tensorflow2/default/1/nli_BiGRU_model.keras\n"
     ]
    }
   ],
   "source": [
    "lstm_path = input(\"Input the path to LSTM model: \")\n",
    "bilstm_path = input(\"Input the path to BiLSTM model: \")\n",
    "gru_path = input(\"Input the path to GRU model: \")\n",
    "bigru_path = input(\"Input the path to BiGRU model: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:08.944584Z",
     "iopub.status.busy": "2025-04-04T00:42:08.944373Z",
     "iopub.status.idle": "2025-04-04T00:42:08.948477Z",
     "shell.execute_reply": "2025-04-04T00:42:08.947473Z",
     "shell.execute_reply.started": "2025-04-04T00:42:08.944565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:08.949557Z",
     "iopub.status.busy": "2025-04-04T00:42:08.949329Z",
     "iopub.status.idle": "2025-04-04T00:42:10.056860Z",
     "shell.execute_reply": "2025-04-04T00:42:10.055981Z",
     "shell.execute_reply.started": "2025-04-04T00:42:08.949525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LSTM\": load_model(\n",
    "        lstm_path,\n",
    "        custom_objects={\"soft_attention\": soft_attention}\n",
    "    ),\n",
    "    \"BiLSTM\": load_model(\n",
    "        bilstm_path,\n",
    "        custom_objects={\"soft_attention\": soft_attention}\n",
    "    ),\n",
    "    \"GRU\": load_model(\n",
    "        gru_path,\n",
    "        custom_objects={\"soft_attention\": soft_attention}\n",
    "    ),\n",
    "    \"BiGRU\": load_model(\n",
    "        bigru_path,\n",
    "        custom_objects={\"soft_attention\": soft_attention}\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model weights for weighted averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:10.058133Z",
     "iopub.status.busy": "2025-04-04T00:42:10.057881Z",
     "iopub.status.idle": "2025-04-04T00:42:19.473561Z",
     "shell.execute_reply": "2025-04-04T00:42:19.472842Z",
     "shell.execute_reply.started": "2025-04-04T00:42:10.058113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the path to model accuracy weights:  /kaggle/input/testing/model_accuracy_weights.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "weights_path = input(\"Input the path to model accuracy weights: \")\n",
    "with open(weights_path, \"r\") as f:\n",
    "    weights = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:19.474581Z",
     "iopub.status.busy": "2025-04-04T00:42:19.474336Z",
     "iopub.status.idle": "2025-04-04T00:42:26.460425Z",
     "shell.execute_reply": "2025-04-04T00:42:26.459668Z",
     "shell.execute_reply.started": "2025-04-04T00:42:19.474560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "weighted_sum = np.zeros((X_test_p.shape[0], 2), dtype=np.float32)\n",
    "    \n",
    "for name, model in models.items():\n",
    "    raw_preds = model.predict([X_test_p, X_test_h])\n",
    "\n",
    "    # Apply weighting and accumulate\n",
    "    weighted_preds = raw_preds * weights[name]\n",
    "    weighted_sum += weighted_preds\n",
    "\n",
    "# Calculate final predictions\n",
    "y_pred = np.argmax(weighted_sum, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions to a csv file with required name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:26.461649Z",
     "iopub.status.busy": "2025-04-04T00:42:26.461310Z",
     "iopub.status.idle": "2025-04-04T00:42:26.465618Z",
     "shell.execute_reply": "2025-04-04T00:42:26.464786Z",
     "shell.execute_reply.started": "2025-04-04T00:42:26.461616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions_only(y_pred, output_path):\n",
    "    \"\"\"Save only the predictions to a CSV file with a 'predictions' column.\"\"\"\n",
    "    df = pd.DataFrame({\"predictions\": y_pred.flatten()})\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:42:26.466548Z",
     "iopub.status.busy": "2025-04-04T00:42:26.466327Z",
     "iopub.status.idle": "2025-04-04T00:42:26.491049Z",
     "shell.execute_reply": "2025-04-04T00:42:26.490170Z",
     "shell.execute_reply.started": "2025-04-04T00:42:26.466517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to Group_47_B.csv\n"
     ]
    }
   ],
   "source": [
    "save_predictions_only(y_pred, \"Group_47_B.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7023853,
     "sourceId": 11241996,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7032281,
     "sourceId": 11253160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7043910,
     "sourceId": 11268612,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7044164,
     "sourceId": 11269004,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7044194,
     "sourceId": 11269042,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 290889,
     "modelInstanceId": 269898,
     "sourceId": 319900,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
