{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Inference using Ensemble Architecture with Attention and Local Inference Modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:57.958439Z",
     "iopub.status.busy": "2025-04-03T20:12:57.958081Z",
     "iopub.status.idle": "2025-04-03T20:12:57.962272Z",
     "shell.execute_reply": "2025-04-03T20:12:57.961525Z",
     "shell.execute_reply.started": "2025-04-03T20:12:57.958410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:57.982444Z",
     "iopub.status.busy": "2025-04-03T20:12:57.982222Z",
     "iopub.status.idle": "2025-04-03T20:12:57.986099Z",
     "shell.execute_reply": "2025-04-03T20:12:57.985261Z",
     "shell.execute_reply.started": "2025-04-03T20:12:57.982425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.000943Z",
     "iopub.status.busy": "2025-04-03T20:12:58.000744Z",
     "iopub.status.idle": "2025-04-03T20:12:58.004176Z",
     "shell.execute_reply": "2025-04-03T20:12:58.003508Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.000925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "MAX_LENGTH = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.030249Z",
     "iopub.status.busy": "2025-04-03T20:12:58.030055Z",
     "iopub.status.idle": "2025-04-03T20:12:58.034063Z",
     "shell.execute_reply": "2025-04-03T20:12:58.033314Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.030232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    \"\"\"Load dataset from CSV file\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[\"premise\"].astype(str).tolist(), df[\"hypothesis\"].astype(str).tolist(), df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = input(\"Input the path to train data: \")\n",
    "val_data_path = input(\"Input the path to validation data: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.044797Z",
     "iopub.status.busy": "2025-04-03T20:12:58.044599Z",
     "iopub.status.idle": "2025-04-03T20:12:58.141038Z",
     "shell.execute_reply": "2025-04-03T20:12:58.140359Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.044779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_premises, train_hypotheses, train_labels = load_data(train_data_path)\n",
    "val_premises, val_hypotheses, val_labels = load_data(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.142591Z",
     "iopub.status.busy": "2025-04-03T20:12:58.142240Z",
     "iopub.status.idle": "2025-04-03T20:12:58.192337Z",
     "shell.execute_reply": "2025-04-03T20:12:58.191545Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.142554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length was chosen from this value\n",
    "premise_lengths = [len(text.split()) for text in train_premises]\n",
    "hypothesis_lengths = [len(text.split()) for text in train_hypotheses]\n",
    "\n",
    "max_length = int(np.percentile(premise_lengths + hypothesis_lengths, 95))\n",
    "max_length = round(max_length / 5) * 5\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.193900Z",
     "iopub.status.busy": "2025-04-03T20:12:58.193691Z",
     "iopub.status.idle": "2025-04-03T20:12:58.204159Z",
     "shell.execute_reply": "2025-04-03T20:12:58.203537Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.193882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(texts, tokenizer, max_length):\n",
    "    \"\"\"Convert text to sequences and pad\"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.205438Z",
     "iopub.status.busy": "2025-04-03T20:12:58.205179Z",
     "iopub.status.idle": "2025-04-03T20:12:59.001866Z",
     "shell.execute_reply": "2025-04-03T20:12:59.001167Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.205417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_premises + train_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:59.002889Z",
     "iopub.status.busy": "2025-04-03T20:12:59.002625Z",
     "iopub.status.idle": "2025-04-03T20:12:59.025053Z",
     "shell.execute_reply": "2025-04-03T20:12:59.024468Z",
     "shell.execute_reply.started": "2025-04-03T20:12:59.002868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:59.026161Z",
     "iopub.status.busy": "2025-04-03T20:12:59.025866Z",
     "iopub.status.idle": "2025-04-03T20:13:00.610091Z",
     "shell.execute_reply": "2025-04-03T20:13:00.609430Z",
     "shell.execute_reply.started": "2025-04-03T20:12:59.026131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_p = tokenize_and_pad(train_premises, tokenizer, MAX_LENGTH)\n",
    "X_train_h = tokenize_and_pad(train_hypotheses, tokenizer, MAX_LENGTH)\n",
    "y_train = to_categorical(train_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:00.611210Z",
     "iopub.status.busy": "2025-04-03T20:13:00.610921Z",
     "iopub.status.idle": "2025-04-03T20:13:00.804329Z",
     "shell.execute_reply": "2025-04-03T20:13:00.803750Z",
     "shell.execute_reply.started": "2025-04-03T20:13:00.611182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_val_p = tokenize_and_pad(val_premises, tokenizer, MAX_LENGTH)\n",
    "X_val_h = tokenize_and_pad(val_hypotheses, tokenizer, MAX_LENGTH)\n",
    "y_val = to_categorical(val_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:00.806842Z",
     "iopub.status.busy": "2025-04-03T20:13:00.806644Z",
     "iopub.status.idle": "2025-04-03T20:13:00.810092Z",
     "shell.execute_reply": "2025-04-03T20:13:00.809219Z",
     "shell.execute_reply.started": "2025-04-03T20:13:00.806824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:00.812232Z",
     "iopub.status.busy": "2025-04-03T20:13:00.811951Z",
     "iopub.status.idle": "2025-04-03T20:13:00.823402Z",
     "shell.execute_reply": "2025-04-03T20:13:00.822676Z",
     "shell.execute_reply.started": "2025-04-03T20:13:00.812205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file, embedding_dim):\n",
    "    \"\"\"\n",
    "    Load the embeddings for every word in the file\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = input(\"Input the path to glove embeddings file: \")\n",
    "embeddings_index = load_glove_embeddings(glove_path, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.387650Z",
     "iopub.status.busy": "2025-04-03T20:13:23.387399Z",
     "iopub.status.idle": "2025-04-03T20:13:23.392017Z",
     "shell.execute_reply": "2025-04-03T20:13:23.391252Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.387629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding_index, word_index, embedding_dim, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix from the GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        if index >= vocab_size:\n",
    "            break\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.393080Z",
     "iopub.status.busy": "2025-04-03T20:13:23.392791Z",
     "iopub.status.idle": "2025-04-03T20:13:23.485337Z",
     "shell.execute_reply": "2025-04-03T20:13:23.484670Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.393051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = create_embedding_matrix(embeddings_index, word_index, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.486356Z",
     "iopub.status.busy": "2025-04-03T20:13:23.486046Z",
     "iopub.status.idle": "2025-04-03T20:13:23.490054Z",
     "shell.execute_reply": "2025-04-03T20:13:23.489362Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.486325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input, Embedding, Bidirectional, GRU, LSTM, Dense, \n",
    "                                    Concatenate, Subtract, Multiply, Dropout, Dot, Softmax)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.490936Z",
     "iopub.status.busy": "2025-04-03T20:13:23.490744Z",
     "iopub.status.idle": "2025-04-03T20:13:23.504076Z",
     "shell.execute_reply": "2025-04-03T20:13:23.503372Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.490918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.505046Z",
     "iopub.status.busy": "2025-04-03T20:13:23.504814Z",
     "iopub.status.idle": "2025-04-03T20:13:23.518303Z",
     "shell.execute_reply": "2025-04-03T20:13:23.517516Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.505027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def soft_attention(premise, hypothesis):\n",
    "    attention = Dot(axes=-1)([premise, hypothesis])\n",
    "    premise_attn = Softmax(axis=-1)(attention)\n",
    "    hypothesis_attn = Softmax(axis=-2)(attention)\n",
    "\n",
    "    premise_aligned = Dot(axes=1)([premise_attn, hypothesis])\n",
    "    hypothesis_aligned = Dot(axes=1)([hypothesis_attn, premise])\n",
    "    return premise_aligned, hypothesis_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:23.519534Z",
     "iopub.status.busy": "2025-04-03T20:13:23.519180Z",
     "iopub.status.idle": "2025-04-03T20:13:23.529499Z",
     "shell.execute_reply": "2025-04-03T20:13:23.528722Z",
     "shell.execute_reply.started": "2025-04-03T20:13:23.519500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_nli_model(rnn_1, rnn_2, vocab_size, embedding_dim, max_length, hidden_dim, embedding_matrix):\n",
    "    premise_input = Input(shape=(max_length,), name=\"premise_input\")\n",
    "    hypothesis_input = Input(shape=(max_length,), name=\"hypothesis_input\")\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    premise_embedded = embedding_layer(premise_input)\n",
    "    hypothesis_embedded = embedding_layer(hypothesis_input)\n",
    "    \n",
    "    # Context Representation using first RNN\n",
    "    premise_encoded = rnn_1(premise_embedded)\n",
    "    hypothesis_encoded = rnn_1(hypothesis_embedded)\n",
    "    \n",
    "    # Soft Attention\n",
    "    premise_aligned, hypothesis_aligned = soft_attention(premise_encoded, hypothesis_encoded)\n",
    "    \n",
    "    # Local Inference Modeling using Difference & Element-wise Product\n",
    "    premise_diff = Subtract()([premise_encoded, premise_aligned])\n",
    "    hypothesis_diff = Subtract()([hypothesis_encoded, hypothesis_aligned])\n",
    "    premise_mul = Multiply()([premise_encoded, premise_aligned])\n",
    "    hypothesis_mul = Multiply()([hypothesis_encoded, hypothesis_aligned])\n",
    "    \n",
    "    premise_combined = Concatenate()([premise_encoded, premise_aligned, premise_diff, premise_mul])\n",
    "    hypothesis_combined = Concatenate()([hypothesis_encoded, hypothesis_aligned, hypothesis_diff, hypothesis_mul])\n",
    "    \n",
    "    # Inference Composition using second RNN\n",
    "    premise_composed = rnn_2(premise_combined)\n",
    "    hypothesis_composed = rnn_2(hypothesis_combined)\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    merged = Concatenate()([premise_composed, hypothesis_composed])\n",
    "    dense = Dense(hidden_dim, activation=\"relu\")(merged)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    dense = Dense(hidden_dim // 2, activation=\"relu\")(dense)\n",
    "    output = Dense(2, activation=\"softmax\")(dense)\n",
    "    \n",
    "    model = Model(inputs=[premise_input, hypothesis_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:28:42.435945Z",
     "iopub.status.busy": "2025-04-03T20:28:42.435652Z",
     "iopub.status.idle": "2025-04-03T20:28:42.919303Z",
     "shell.execute_reply": "2025-04-03T20:28:42.918610Z",
     "shell.execute_reply.started": "2025-04-03T20:28:42.435923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LSTM\": build_nli_model(\n",
    "        LSTM(HIDDEN_DIM, return_sequences=True),\n",
    "        LSTM(HIDDEN_DIM, return_sequences=False),\n",
    "        VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, HIDDEN_DIM, embedding_matrix\n",
    "    ),\n",
    "    \"BiLSTM\": build_nli_model(\n",
    "        Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True)),\n",
    "        Bidirectional(LSTM(HIDDEN_DIM, return_sequences=False)),\n",
    "        VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, HIDDEN_DIM, embedding_matrix\n",
    "    ),\n",
    "    \"GRU\": build_nli_model(\n",
    "        GRU(HIDDEN_DIM, return_sequences=True),\n",
    "        GRU(HIDDEN_DIM, return_sequences=False),\n",
    "        VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, HIDDEN_DIM, embedding_matrix\n",
    "    ),\n",
    "    \"BiGRU\": build_nli_model(\n",
    "        Bidirectional(GRU(HIDDEN_DIM, return_sequences=True)),\n",
    "        Bidirectional(GRU(HIDDEN_DIM, return_sequences=False)),\n",
    "        VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, HIDDEN_DIM, embedding_matrix\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:28:42.920548Z",
     "iopub.status.busy": "2025-04-03T20:28:42.920323Z",
     "iopub.status.idle": "2025-04-03T20:28:42.936479Z",
     "shell.execute_reply": "2025-04-03T20:28:42.935674Z",
     "shell.execute_reply.started": "2025-04-03T20:28:42.920528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:28:42.942815Z",
     "iopub.status.busy": "2025-04-03T20:28:42.942621Z",
     "iopub.status.idle": "2025-04-03T20:42:25.174106Z",
     "shell.execute_reply": "2025-04-03T20:42:25.173163Z",
     "shell.execute_reply.started": "2025-04-03T20:28:42.942799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.5597 - loss: 0.6766 - val_accuracy: 0.6444 - val_loss: 0.6217\n",
      "Epoch 2/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6556 - loss: 0.6153 - val_accuracy: 0.6599 - val_loss: 0.6083\n",
      "Epoch 3/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6680 - loss: 0.6021 - val_accuracy: 0.6620 - val_loss: 0.6006\n",
      "Epoch 4/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6819 - loss: 0.5850 - val_accuracy: 0.6672 - val_loss: 0.5953\n",
      "Epoch 5/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6903 - loss: 0.5705 - val_accuracy: 0.6709 - val_loss: 0.5917\n",
      "Epoch 6/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7047 - loss: 0.5526 - val_accuracy: 0.6755 - val_loss: 0.5896\n",
      "Epoch 7/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7139 - loss: 0.5447 - val_accuracy: 0.6798 - val_loss: 0.5895\n",
      "Epoch 8/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7245 - loss: 0.5258 - val_accuracy: 0.6863 - val_loss: 0.5877\n",
      "Epoch 9/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7402 - loss: 0.5085 - val_accuracy: 0.6851 - val_loss: 0.5858\n",
      "Epoch 10/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7531 - loss: 0.4919 - val_accuracy: 0.6878 - val_loss: 0.6006\n",
      "Epoch 11/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7551 - loss: 0.4815 - val_accuracy: 0.6893 - val_loss: 0.5977\n",
      "Epoch 12/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7695 - loss: 0.4642 - val_accuracy: 0.6918 - val_loss: 0.6248\n",
      "Epoch 13/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7833 - loss: 0.4416 - val_accuracy: 0.6808 - val_loss: 0.6504\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 33ms/step - accuracy: 0.5807 - loss: 0.6682 - val_accuracy: 0.6492 - val_loss: 0.6134\n",
      "Epoch 2/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.6707 - loss: 0.6026 - val_accuracy: 0.6645 - val_loss: 0.6020\n",
      "Epoch 3/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.6789 - loss: 0.5893 - val_accuracy: 0.6672 - val_loss: 0.5990\n",
      "Epoch 4/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.7016 - loss: 0.5657 - val_accuracy: 0.6765 - val_loss: 0.5814\n",
      "Epoch 5/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.7273 - loss: 0.5367 - val_accuracy: 0.6876 - val_loss: 0.5836\n",
      "Epoch 6/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.7339 - loss: 0.5172 - val_accuracy: 0.6865 - val_loss: 0.5771\n",
      "Epoch 7/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.7461 - loss: 0.4988 - val_accuracy: 0.6869 - val_loss: 0.5833\n",
      "Epoch 8/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.7743 - loss: 0.4635 - val_accuracy: 0.6841 - val_loss: 0.6070\n",
      "Epoch 9/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.7945 - loss: 0.4280 - val_accuracy: 0.6851 - val_loss: 0.6154\n",
      "Epoch 10/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8127 - loss: 0.4026 - val_accuracy: 0.6899 - val_loss: 0.6281\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.5210 - loss: 0.6911 - val_accuracy: 0.5693 - val_loss: 0.6683\n",
      "Epoch 2/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.5841 - loss: 0.6612 - val_accuracy: 0.6511 - val_loss: 0.6169\n",
      "Epoch 3/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6585 - loss: 0.6136 - val_accuracy: 0.6583 - val_loss: 0.6081\n",
      "Epoch 4/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.6734 - loss: 0.5964 - val_accuracy: 0.6534 - val_loss: 0.6041\n",
      "Epoch 5/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.6857 - loss: 0.5795 - val_accuracy: 0.6569 - val_loss: 0.6005\n",
      "Epoch 6/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.6998 - loss: 0.5650 - val_accuracy: 0.6730 - val_loss: 0.5889\n",
      "Epoch 7/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7059 - loss: 0.5530 - val_accuracy: 0.6721 - val_loss: 0.5899\n",
      "Epoch 8/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7156 - loss: 0.5407 - val_accuracy: 0.6761 - val_loss: 0.5910\n",
      "Epoch 9/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7235 - loss: 0.5323 - val_accuracy: 0.6830 - val_loss: 0.5905\n",
      "Epoch 10/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7314 - loss: 0.5222 - val_accuracy: 0.6773 - val_loss: 0.5878\n",
      "Epoch 11/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7441 - loss: 0.5080 - val_accuracy: 0.6802 - val_loss: 0.5917\n",
      "Epoch 12/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.7496 - loss: 0.4939 - val_accuracy: 0.6844 - val_loss: 0.5992\n",
      "Epoch 13/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7606 - loss: 0.4747 - val_accuracy: 0.6795 - val_loss: 0.6058\n",
      "Epoch 14/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.7748 - loss: 0.4606 - val_accuracy: 0.6813 - val_loss: 0.6579\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 31ms/step - accuracy: 0.5687 - loss: 0.6773 - val_accuracy: 0.6577 - val_loss: 0.6152\n",
      "Epoch 2/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.6663 - loss: 0.6056 - val_accuracy: 0.6605 - val_loss: 0.6016\n",
      "Epoch 3/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.6844 - loss: 0.5840 - val_accuracy: 0.6691 - val_loss: 0.5939\n",
      "Epoch 4/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7028 - loss: 0.5657 - val_accuracy: 0.6770 - val_loss: 0.5910\n",
      "Epoch 5/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7192 - loss: 0.5438 - val_accuracy: 0.6874 - val_loss: 0.5871\n",
      "Epoch 6/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7344 - loss: 0.5237 - val_accuracy: 0.6848 - val_loss: 0.5819\n",
      "Epoch 7/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7501 - loss: 0.5041 - val_accuracy: 0.6948 - val_loss: 0.5928\n",
      "Epoch 8/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7744 - loss: 0.4699 - val_accuracy: 0.6951 - val_loss: 0.5902\n",
      "Epoch 9/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7893 - loss: 0.4437 - val_accuracy: 0.6961 - val_loss: 0.6059\n",
      "Epoch 10/20\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.8032 - loss: 0.4165 - val_accuracy: 0.6991 - val_loss: 0.6268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_accuracies = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        f\"nli_{name}_model.keras\", \n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True, \n",
    "        mode=\"max\",\n",
    "        verbose=0\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_p, X_train_h], y_train, \n",
    "        batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([X_val_p, X_val_h], y_val),\n",
    "        callbacks=[checkpoint_callback, early_stop_callback]\n",
    "    )\n",
    "    \n",
    "    best_val_acc = max(history.history[\"val_accuracy\"])\n",
    "    val_accuracies[name] = best_val_acc\n",
    "\n",
    "    model.load_weights(f\"nli_{name}_model.keras\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total_acc = sum(val_accuracies.values())\n",
    "weights = {name: acc/total_acc for name, acc in val_accuracies.items()}\n",
    "\n",
    "with open(\"model_accuracy_weights.json\", \"w\") as f:\n",
    "    json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:42:25.198304Z",
     "iopub.status.busy": "2025-04-03T20:42:25.198096Z",
     "iopub.status.idle": "2025-04-03T20:42:35.125890Z",
     "shell.execute_reply": "2025-04-03T20:42:35.125000Z",
     "shell.execute_reply.started": "2025-04-03T20:42:25.198269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "weighted_sum = np.zeros_like(y_val, dtype=np.float32)\n",
    "    \n",
    "for name, model in models.items():\n",
    "    raw_preds = model.predict([X_val_p, X_val_h])\n",
    "\n",
    "    # Apply weighting and accumulate\n",
    "    weighted_preds = raw_preds * weights[name]\n",
    "    weighted_sum += weighted_preds\n",
    "\n",
    "# Calculate final predictions\n",
    "y_pred = np.argmax(weighted_sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:42:35.127869Z",
     "iopub.status.busy": "2025-04-03T20:42:35.127630Z",
     "iopub.status.idle": "2025-04-03T20:42:35.141504Z",
     "shell.execute_reply": "2025-04-03T20:42:35.140597Z",
     "shell.execute_reply.started": "2025-04-03T20:42:35.127848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7185    0.6636    0.6900      3258\n",
      "           1     0.7059    0.7565    0.7303      3478\n",
      "\n",
      "    accuracy                         0.7115      6736\n",
      "   macro avg     0.7122    0.7100    0.7101      6736\n",
      "weighted avg     0.7120    0.7115    0.7108      6736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:42:35.142565Z",
     "iopub.status.busy": "2025-04-03T20:42:35.142256Z",
     "iopub.status.idle": "2025-04-03T20:42:35.146334Z",
     "shell.execute_reply": "2025-04-03T20:42:35.145539Z",
     "shell.execute_reply.started": "2025-04-03T20:42:35.142535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions_only(y_pred, output_path):\n",
    "    \"\"\"Save only the predictions to a CSV file with a 'predictions' column.\"\"\"\n",
    "    df = pd.DataFrame({\"predictions\": y_pred.flatten()})\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:42:35.147596Z",
     "iopub.status.busy": "2025-04-03T20:42:35.147259Z",
     "iopub.status.idle": "2025-04-03T20:42:35.165814Z",
     "shell.execute_reply": "2025-04-03T20:42:35.165156Z",
     "shell.execute_reply.started": "2025-04-03T20:42:35.147564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to Group_47_B.csv.predict\n"
     ]
    }
   ],
   "source": [
    "save_predictions_only(y_pred, \"Group_47_B.csv.predict\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7023853,
     "sourceId": 11241996,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7032281,
     "sourceId": 11253160,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
