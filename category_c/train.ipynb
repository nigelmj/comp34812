{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:22.326241Z",
     "iopub.status.busy": "2025-04-03T14:02:22.325796Z",
     "iopub.status.idle": "2025-04-03T14:02:25.318696Z",
     "shell.execute_reply": "2025-04-03T14:02:25.318036Z",
     "shell.execute_reply.started": "2025-04-03T14:02:22.326201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:25.320069Z",
     "iopub.status.busy": "2025-04-03T14:02:25.319655Z",
     "iopub.status.idle": "2025-04-03T14:02:25.365315Z",
     "shell.execute_reply": "2025-04-03T14:02:25.364280Z",
     "shell.execute_reply.started": "2025-04-03T14:02:25.320046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:25.367074Z",
     "iopub.status.busy": "2025-04-03T14:02:25.366828Z",
     "iopub.status.idle": "2025-04-03T14:02:25.378868Z",
     "shell.execute_reply": "2025-04-03T14:02:25.378165Z",
     "shell.execute_reply.started": "2025-04-03T14:02:25.367052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "random.seed(a=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:25.380151Z",
     "iopub.status.busy": "2025-04-03T14:02:25.379915Z",
     "iopub.status.idle": "2025-04-03T14:02:43.099193Z",
     "shell.execute_reply": "2025-04-03T14:02:43.098232Z",
     "shell.execute_reply.started": "2025-04-03T14:02:25.380132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer, get_scheduler, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:43.100800Z",
     "iopub.status.busy": "2025-04-03T14:02:43.100170Z",
     "iopub.status.idle": "2025-04-03T14:02:47.198198Z",
     "shell.execute_reply": "2025-04-03T14:02:47.197407Z",
     "shell.execute_reply.started": "2025-04-03T14:02:43.100767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cf38929b0445f7917caea5da4a9cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadb2679c7c14a4abbc32fdb9b898c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2b8bbe2d154a35bfcd30711c62d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ca9420418745358155b04bdbf18348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c8832904b344d0901d67968f047f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.199428Z",
     "iopub.status.busy": "2025-04-03T14:02:47.199120Z",
     "iopub.status.idle": "2025-04-03T14:02:47.203268Z",
     "shell.execute_reply": "2025-04-03T14:02:47.202051Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.199398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to handle the datasets and split them into columns and process them to the right size and add paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.204239Z",
     "iopub.status.busy": "2025-04-03T14:02:47.203956Z",
     "iopub.status.idle": "2025-04-03T14:02:47.218082Z",
     "shell.execute_reply": "2025-04-03T14:02:47.217212Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.204218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.encodings = tokenizer(list(data['premise']), list(data['hypothesis']), padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "        self.labels = torch.tensor(data['label'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to learn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.219342Z",
     "iopub.status.busy": "2025-04-03T14:02:47.219040Z",
     "iopub.status.idle": "2025-04-03T14:02:47.233452Z",
     "shell.execute_reply": "2025-04-03T14:02:47.232641Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.219314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DNNTransformerModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(DNNTransformerModel, self).__init__()\n",
    "        self.transformer = RobertaModel.from_pretrained(model_name)\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(self.transformer.config.hidden_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.dnn(hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.236024Z",
     "iopub.status.busy": "2025-04-03T14:02:47.235767Z",
     "iopub.status.idle": "2025-04-03T14:02:47.246212Z",
     "shell.execute_reply": "2025-04-03T14:02:47.245530Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.236003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.247605Z",
     "iopub.status.busy": "2025-04-03T14:02:47.247382Z",
     "iopub.status.idle": "2025-04-03T14:02:47.439646Z",
     "shell.execute_reply": "2025-04-03T14:02:47.438948Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.247586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/trained/train.csv') \n",
    "val_data = pd.read_csv('/kaggle/input/trained/dev.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:47.440762Z",
     "iopub.status.busy": "2025-04-03T14:02:47.440442Z",
     "iopub.status.idle": "2025-04-03T14:02:49.768649Z",
     "shell.execute_reply": "2025-04-03T14:02:49.768001Z",
     "shell.execute_reply.started": "2025-04-03T14:02:47.440731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracing Nouns and Verbs from the sets of premise and hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:49.769578Z",
     "iopub.status.busy": "2025-04-03T14:02:49.769350Z",
     "iopub.status.idle": "2025-04-03T14:02:50.526969Z",
     "shell.execute_reply": "2025-04-03T14:02:50.526291Z",
     "shell.execute_reply.started": "2025-04-03T14:02:49.769558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_nouns_verbs(textP, textH):\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    for sent in textP:\n",
    "        doc = nlp(sent)\n",
    "        nouns.extend(token.text for token in doc if token.pos_ == \"NOUN\")\n",
    "        verbs.extend(token.text for token in doc if token.pos_ == \"VERB\")\n",
    "    for sent in textH:\n",
    "        doc = nlp(sent)\n",
    "        nouns.extend(token.text for token in doc if token.pos_ == \"NOUN\")\n",
    "        verbs.extend(token.text for token in doc if token.pos_ == \"VERB\")\n",
    "    return nouns, verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional data based on research papers to improve accuracy (explained in README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:02:50.527907Z",
     "iopub.status.busy": "2025-04-03T14:02:50.527687Z",
     "iopub.status.idle": "2025-04-03T14:08:08.017148Z",
     "shell.execute_reply": "2025-04-03T14:08:08.016233Z",
     "shell.execute_reply.started": "2025-04-03T14:02:50.527889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting\n",
      "Extracted\n",
      "adding new data\n",
      "new data added\n"
     ]
    }
   ],
   "source": [
    "premise = train_data['premise']\n",
    "hypothesis = train_data['hypothesis']\n",
    "print(\"Extracting\")\n",
    "nouns, verbs = extract_nouns_verbs(premise, hypothesis)\n",
    "print(\"Extracted\")\n",
    "new_training_data = pd.DataFrame(columns=[\"premise\", \"hypothesis\", \"label\"])\n",
    "print(\"adding new data\")\n",
    "for i in range(1000):\n",
    "    n1, n2 = random.sample(nouns, 2)\n",
    "    v = random.choice(verbs)\n",
    "    sent1 = f\"The {n1} {v} the {n2}\"\n",
    "    sent2 = f\"The {n1} does not {v} the {n2}\"\n",
    "    num = random.randint(1, 10)\n",
    "    if num <= 3:\n",
    "        new_training_data.loc[len(new_training_data)] = [sent1, sent1, 1]\n",
    "    new_training_data.loc[len(new_training_data)] = [sent2, sent1, 0]\n",
    "\n",
    "train_data = pd.concat([train_data, new_training_data], ignore_index=True)\n",
    "print(\"new data added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:08.018379Z",
     "iopub.status.busy": "2025-04-03T14:08:08.018084Z",
     "iopub.status.idle": "2025-04-03T14:08:08.022181Z",
     "shell.execute_reply": "2025-04-03T14:08:08.021316Z",
     "shell.execute_reply.started": "2025-04-03T14:08:08.018350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:08.023465Z",
     "iopub.status.busy": "2025-04-03T14:08:08.023184Z",
     "iopub.status.idle": "2025-04-03T14:08:13.754029Z",
     "shell.execute_reply": "2025-04-03T14:08:13.753313Z",
     "shell.execute_reply.started": "2025-04-03T14:08:08.023436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_data)\n",
    "val_dataset = NLIDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:13.755092Z",
     "iopub.status.busy": "2025-04-03T14:08:13.754791Z",
     "iopub.status.idle": "2025-04-03T14:08:16.474471Z",
     "shell.execute_reply": "2025-04-03T14:08:16.473513Z",
     "shell.execute_reply.started": "2025-04-03T14:08:13.755062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d77534409143cb8863dda56e3d6ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DNNTransformerModel(MODEL_NAME, num_labels=2)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:16.475675Z",
     "iopub.status.busy": "2025-04-03T14:08:16.475427Z",
     "iopub.status.idle": "2025-04-03T14:08:16.479300Z",
     "shell.execute_reply": "2025-04-03T14:08:16.478391Z",
     "shell.execute_reply.started": "2025-04-03T14:08:16.475652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:16.480529Z",
     "iopub.status.busy": "2025-04-03T14:08:16.480206Z",
     "iopub.status.idle": "2025-04-03T14:08:16.493686Z",
     "shell.execute_reply": "2025-04-03T14:08:16.492835Z",
     "shell.execute_reply.started": "2025-04-03T14:08:16.480498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:16.494874Z",
     "iopub.status.busy": "2025-04-03T14:08:16.494636Z",
     "iopub.status.idle": "2025-04-03T14:08:16.505472Z",
     "shell.execute_reply": "2025-04-03T14:08:16.504753Z",
     "shell.execute_reply.started": "2025-04-03T14:08:16.494853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "WARMUP_RATIO = 0.1\n",
    "EPOCHS = 1\n",
    "NUM_TRAINING_STEPS = len(train_loader) * EPOCHS\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=WARMUP_RATIO, num_training_steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:16.506518Z",
     "iopub.status.busy": "2025-04-03T14:08:16.506242Z",
     "iopub.status.idle": "2025-04-03T14:08:16.516408Z",
     "shell.execute_reply": "2025-04-03T14:08:16.515767Z",
     "shell.execute_reply.started": "2025-04-03T14:08:16.506490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:08:16.517500Z",
     "iopub.status.busy": "2025-04-03T14:08:16.517221Z",
     "iopub.status.idle": "2025-04-03T14:18:41.590446Z",
     "shell.execute_reply": "2025-04-03T14:18:41.589515Z",
     "shell.execute_reply.started": "2025-04-03T14:08:16.517479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 1607/1607 [10:25<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.4032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    loop.set_description(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:18:41.591727Z",
     "iopub.status.busy": "2025-04-03T14:18:41.591393Z",
     "iopub.status.idle": "2025-04-03T14:18:42.909001Z",
     "shell.execute_reply": "2025-04-03T14:18:42.908018Z",
     "shell.execute_reply.started": "2025-04-03T14:18:41.591695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"nli_dnn_transformer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6991204,
     "sourceId": 11197764,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6991231,
     "sourceId": 11197802,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 284425,
     "modelInstanceId": 263317,
     "sourceId": 310401,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 285314,
     "modelInstanceId": 264221,
     "sourceId": 311528,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
